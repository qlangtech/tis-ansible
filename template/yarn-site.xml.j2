<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>

<!-- Site specific YARN configuration properties -->
  <!--RM的主机名 -->
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>{{ groups['hadoop-yarn-resource-manager'][0] }}</value>
  </property>

  <!--RM对客户端暴露的地址,客户端通过该地址向RM提交应用程序、杀死应用程序等-->
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>${yarn.resourcemanager.hostname}:8032</value>
  </property>

  <!--RM对AM暴露的访问地址,AM通过该地址向RM申请资源、释放资源等-->
  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>${yarn.resourcemanager.hostname}:8030</value>
  </property>

  <!--RM对外暴露的web http地址,用户可通过该地址在浏览器中查看集群信息-->
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>${yarn.resourcemanager.hostname}:8088</value>
  </property>

  <!--RM对NM暴露地址,NM通过该地址向RM汇报心跳、领取任务等-->
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>${yarn.resourcemanager.hostname}:8031</value>
  </property>

  <!--RM对管理员暴露的访问地址,管理员通过该地址向RM发送管理命令等-->
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>${yarn.resourcemanager.hostname}:8033</value>
  </property>

  <!--启用的资源调度器主类。目前可用的有FIFO、Capacity Scheduler和Fair Scheduler-->
  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler</value>
  </property>

  {% if hadoop_log_dirs is defined %}
  <property>
    <name>yarn.nodemanager.log-dirs</name>
    <value>{{ hadoop_log_dirs }}</value>
  </property>
  {% endif %}

  <!--单个容器可申请的最小/最大虚拟CPU个数。比如设置为1和4，则运行MapRedce作业时，每个Task最少可申请1个虚拟CPU，最多可申请4个虚拟CPU-->
  <property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>8</value>
  </property>
  <!--NM运行的Container,总的可用虚拟CPU个数,默认值8-->
  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>8</value>
  </property>

  <!--单个容器可申请的最小与最大内存，应用在运行申请内存时不能超过最大值，小于最小值则分配最小值-->
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>2048</value>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>{{ (yarn_nodemanager_resource_memorymb*0.8)|int }}</value>
  </property>

  <!--NM运行的Container,可以分配到的物理内存,一旦设置,运行过程中不可动态修改,默认8192MB-->
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>{{ yarn_nodemanager_resource_memorymb }}</value>
  </property>

  <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true-->
  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>true</value>
  </property>

  <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true-->
  <property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>true</value>
  </property>

  <!--每使用1MB物理内存，最多可用的虚拟内存数,默认值2.1-->
  <property>
    <name>yarn.nodemanager.vmem-pmem-ratio</name>
    <value>2.1</value>
  </property>

  <!--一块磁盘的最高使用率，当一块磁盘的使用率超过该值时，则认为该盘为坏盘，不再使用该盘，默认是100，表示100%-->
  <property>
    <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
    <value>98.0</value>
  </property>

  <!--NM运行的附属服务,需配置成mapreduce_shuffle,才可运行MapReduce程序，如果需要就配置spark shuffle-->
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle{% if need_install_spark_shuffle %},spark_shuffle{% endif %}</value>
  </property>

  <!--为了能够运行MapReduce程序，需要让各个NodeManager在启动时加载shuffle server-->
  <property>
    <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>

  {% if need_install_spark_shuffle %}
  <!--配置 spark_shuffle 的class和服务端口-->
  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
    <value>org.apache.spark.network.yarn.YarnShuffleService</value>
  </property>
  <property>
    <name>spark.shuffle.service.port</name>
    <value>7337</value>
  </property>
  {% endif %}


  <!--YARN kerberos security-->
{% if kerberos.enable %}
   <property>
    <name>yarn.resourcemanager.keytab</name>
    <value>{{kerberos.yarn.keytab}}</value>
   </property>

   <property>
    <name>yarn.resourcemanager.principal</name>
    <value>{{kerberos.yarn.principal}}</value>
   </property>

   <property>
    <name>yarn.nodemanager.keytab</name>
    <value>{{kerberos.yarn.keytab}}</value>
   </property>

   <property>
    <name>yarn.nodemanager.principal</name>
    <value>{{kerberos.yarn.principal}}</value>
   </property>
{% endif %}
</configuration>
